{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beenhachi/NPL/blob/main/Natural_Language_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsz_a3s7_9Ha"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import re\n",
        "def load_doc(filename):\n",
        "# open the file as read only\n",
        "    file = open(filename, 'r' )\n",
        "# read all text\n",
        "    text = file.read()\n",
        "# close the file\n",
        "    file.close()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqNcJUAq_9H0"
      },
      "outputs": [],
      "source": [
        "def clean_doc(doc):\n",
        "# split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
        "# remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "# filter out stop words\n",
        "    stop_words = set(stopwords.words( 'english' ))\n",
        "    tokens = [w for w in tokens if not w in stop_words]\n",
        "# filter out short tokens\n",
        "    tokens = [word for word in tokens if len(word) > 1]\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evBDYu8y_9H6"
      },
      "outputs": [],
      "source": [
        "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
        "text = load_doc(filename)\n",
        "tokens = clean_doc(text)\n",
        "# print(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN443DGa_9IA"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "# load doc and add to vocab\n",
        "def add_doc_to_vocab(filename, vocab):\n",
        "# load doc\n",
        "    doc = load_doc(filename)\n",
        "# clean doc\n",
        "    tokens = clean_doc(doc)\n",
        "# update counts\n",
        "    vocab.update(tokens)\n",
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab):\n",
        "# walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "        if filename.startswith( 'cv9' ):\n",
        "            continue\n",
        "# create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "# add doc to vocab\n",
        "        add_doc_to_vocab(path, vocab)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQSTn9lQ_9IG",
        "outputId": "6b06bf80-4192-4b76-9da7-b0774c9d11f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "36053\n"
          ]
        }
      ],
      "source": [
        "# define vocab\n",
        "vocab = Counter()\n",
        "# add all docs to vocab\n",
        "process_docs( 'txt_sentoken/pos' , vocab)\n",
        "process_docs( 'txt_sentoken/neg' , vocab)\n",
        "# print the size of the vocab\n",
        "print(len(vocab))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-2rrgDM_9IO",
        "outputId": "9e266801-777d-4469-dcb1-17aa18904f59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('film', 7974), ('one', 4939), ('movie', 4815), ('like', 3193), ('even', 2261), ('good', 2073), ('time', 2039), ('story', 1899), ('would', 1843), ('much', 1823), ('also', 1757), ('get', 1723), ('character', 1699), ('two', 1642), ('characters', 1618), ('first', 1586), ('see', 1553), ('way', 1515), ('well', 1477), ('make', 1418), ('really', 1400), ('little', 1347), ('films', 1338), ('life', 1329), ('plot', 1286), ('people', 1267), ('could', 1248), ('bad', 1246), ('scene', 1240), ('never', 1197), ('best', 1176), ('new', 1139), ('scenes', 1132), ('many', 1129), ('man', 1122), ('know', 1092), ('movies', 1027), ('great', 1011), ('another', 992), ('action', 980), ('love', 975), ('us', 967), ('go', 950), ('director', 947), ('something', 944), ('end', 943), ('still', 935), ('seems', 930), ('back', 921), ('made', 911)]\n",
            "13069\n"
          ]
        }
      ],
      "source": [
        "# # print the top words in the vocab\n",
        "print(vocab.most_common(50))\n",
        "# # keep tokens with a min occurrence\n",
        "min_occurane = 5\n",
        "tokens = [k for k,c in vocab.items() if c >= min_occurane]\n",
        "print(len(tokens))\n",
        "def save_list(lines, filename):\n",
        "    data = ' \\n ' .join(lines)\n",
        "# open file\n",
        "    file = open(filename, 'w' )\n",
        "# write text\n",
        "    file.write(data)\n",
        "# close file\n",
        "    file.close()\n",
        "\n",
        "# # save tokens to a vocabulary file\n",
        "save_list(tokens, 'vocab.txt' )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9POHUJW_9IS"
      },
      "outputs": [],
      "source": [
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3J0SKTC_9IX"
      },
      "outputs": [],
      "source": [
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc, vocab):\n",
        "# split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "# prepare regex for char filtering\n",
        "    re_punc = re.compile( ' [%s] ' % re.escape(string.punctuation))\n",
        "# remove punctuation from each word\n",
        "    tokens = [re_punc.sub( '' , w) for w in tokens]\n",
        "# filter out tokens not in vocab\n",
        "    tokens = [w for w in tokens if w in vocab]\n",
        "    tokens = ' ' .join(tokens)\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHmD4IKS_9Ib"
      },
      "outputs": [],
      "source": [
        "# load all docs in a directory\n",
        "def process_docs(directory, vocab, is_train):\n",
        "    documents = list()\n",
        "# walk through all files in the folder\n",
        "    for filename in listdir(directory):\n",
        "# skip any reviews in the test set\n",
        "        if is_train and filename.startswith( 'cv9' ):\n",
        "            continue\n",
        "        if not is_train and not filename.startswith( 'cv9' ):\n",
        "            continue\n",
        "# create the full path of the file to open\n",
        "        path = directory + '/' + filename\n",
        "# load the doc\n",
        "        doc = load_doc(path)\n",
        "# clean doc\n",
        "        tokens = clean_doc(doc, vocab)\n",
        "# add to list\n",
        "        documents.append(tokens)\n",
        "    return documents\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuM5fvxn_9Ig",
        "outputId": "7e5354ec-8987-4793-c0f2-b3e242a8cda5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using Theano backend.\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Embedding\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "def load_clean_dataset(vocab, is_train):\n",
        "    neg = process_docs( 'txt_sentoken/neg' , vocab, is_train)\n",
        "    pos = process_docs( 'txt_sentoken/pos' , vocab, is_train)\n",
        "    docs = neg + pos\n",
        "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
        "    return docs, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW-XsIyX_9Im"
      },
      "outputs": [],
      "source": [
        "def create_tokenizer(lines):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(lines)\n",
        "    return tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebt52J8M_9Io"
      },
      "outputs": [],
      "source": [
        "def encode_docs(tokenizer, max_length, docs):\n",
        "# integer encode\n",
        "    encoded = tokenizer.texts_to_sequences(docs)\n",
        "# pad sequences\n",
        "    padded = pad_sequences(encoded, maxlen=max_length, padding= 'post' )\n",
        "    return padded\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_k-O3swF_9Iq"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "def define_model(vocab_size, max_length):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
        "    model.add(Conv1D(filters=32, kernel_size=8, activation= 'relu' ))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10, activation= 'relu'))\n",
        "    model.add(Dense(1, activation= 'sigmoid' ))\n",
        "# compile network\n",
        "    model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
        "# summarize defined model\n",
        "    model.summary()\n",
        "    plot_model(model, to_file= 'model.png' , show_shapes=True)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b64paTF6_9Ir",
        "outputId": "1288fef8-9ede-48dc-e90d-c211bc40625d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Maximum length: 1180 \n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 1180, 100)         1307000   \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 1173, 32)          25632     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 586, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18752)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                187530    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 1,520,173\n",
            "Trainable params: 1,520,173\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# load the vocabulary\n",
        "vocab_filename = 'vocab.txt'\n",
        "vocab = load_doc(vocab_filename)\n",
        "vocab = set(vocab.split())\n",
        "# load training data\n",
        "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
        "# create the tokenizer\n",
        "tokenizer = create_tokenizer(train_docs)\n",
        "# # define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# print( ' Vocabulary size: %d ' % vocab_size)\n",
        "# # calculate the maximum sequence length\n",
        "max_length = max([len(s.split()) for s in train_docs])\n",
        "print( ' Maximum length: %d ' % max_length)\n",
        "# encode data\n",
        "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
        "# define model\n",
        "model = define_model(vocab_size, max_length)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUd7NAVq_9It",
        "outputId": "462e32ae-b017-4f67-e294-14b1ddf57b40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " - 56s - loss: 0.6912 - acc: 0.5222\n",
            "Epoch 2/10\n",
            " - 53s - loss: 0.6114 - acc: 0.6606\n",
            "Epoch 3/10\n",
            " - 55s - loss: 0.2233 - acc: 0.9100\n",
            "Epoch 4/10\n",
            " - 52s - loss: 0.0302 - acc: 0.9967\n",
            "Epoch 5/10\n",
            " - 53s - loss: 0.0050 - acc: 1.0000\n",
            "Epoch 6/10\n",
            " - 54s - loss: 0.0024 - acc: 1.0000\n",
            "Epoch 7/10\n",
            " - 52s - loss: 0.0015 - acc: 1.0000\n",
            "Epoch 8/10\n",
            " - 53s - loss: 0.0011 - acc: 1.0000\n",
            "Epoch 9/10\n",
            " - 48s - loss: 8.6973e-04 - acc: 1.0000\n",
            "Epoch 10/10\n",
            " - 49s - loss: 6.9222e-04 - acc: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f105a46dbe0>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0W9i47fR_9Iu"
      },
      "outputs": [],
      "source": [
        "model.save( 'model.h5' )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjE5-8iP_9Iw",
        "outputId": "83160b09-386d-4122-f4e0-ee2e93e82320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Test Accuracy: 87.500000 \n"
          ]
        }
      ],
      "source": [
        "# load all reviews\n",
        "from keras.models import load_model\n",
        "\n",
        "test_docs, ytest = load_clean_dataset(vocab, False)\n",
        "# create the tokenizer\n",
        "# encode data\n",
        "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
        "model = load_model( 'model.h5' )\n",
        "# _, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
        "# print( ' Train Accuracy: %f ' % (acc*100))\n",
        "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
        "print( ' Test Accuracy: %f ' % (acc*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npiF6u1r_9Ix"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import re\n",
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "# classify a review as negative or positive\n",
        "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
        "# clean review\n",
        "    line = clean_doc(review, vocab)\n",
        "# encode and pad review\n",
        "    padded = encode_docs(tokenizer, max_length, [line])\n",
        "# predict sentiment\n",
        "    yhat = model.predict(padded, verbose=0)\n",
        "# retrieve predicted percentage and label\n",
        "    percent_pos = yhat[0,0]\n",
        "    if round(percent_pos) == 0:\n",
        "        return (1-percent_pos), 'NEGATIVE'\n",
        "    return percent_pos, 'POSITIVE'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvgBgmB5_9Iy",
        "outputId": "f29b8788-d89c-4bf3-ab38-a09753a46f20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Review: [ Everyone will enjoy this film. I love it, recommended! ]\n",
            "Sentiment: NEGATIVE (50.973%) \n",
            " Review: [ This is a bad movie. Do not watch it. It sucks. ]\n",
            "Sentiment: NEGATIVE (52.625%) \n"
          ]
        }
      ],
      "source": [
        "text = ' Everyone will enjoy this film. I love it, recommended! '\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print( ' Review: [%s]\\nSentiment: %s (%.3f%%) ' % (text, sentiment, percent*100))\n",
        "# test negative text\n",
        "text = ' This is a bad movie. Do not watch it. It sucks. '\n",
        "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
        "print( ' Review: [%s]\\nSentiment: %s (%.3f%%) ' % (text, sentiment, percent*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0VSL4YS_9Iz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}